{"cells":[{"cell_type":"markdown","metadata":{"id":"zbD2ITIP3Nm-"},"source":["Colab Link: https://colab.research.google.com/drive/1tU4Leou1F6XDVlBaLfBphWTL43-BPV_d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zB1YLJ65Z03W"},"outputs":[],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = \"cs231n-project\"\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"]},{"cell_type":"markdown","metadata":{"id":"hM_SU5thbA_d"},"source":["# Load Dataset and create Train/Test Split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3r7OvXHaEWW"},"outputs":[],"source":["dataset_path = f\"/content/drive/My Drive/cs231n-project/dataset/Taskent\"\n","\n","dataset_size = 21\n","test_indices = [1,2]\n","train_indices = [i for i in range(dataset_size) if not i in test_indices]\n","# train_indices = [0,3,4]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2U0skZZcje9"},"outputs":[],"source":["from image_dataset import *\n","\n","\n","train_dataset = PetroSubImageDataset(dataset_path, image_indices=train_indices)\n","test_dataset = PetroSubImageDataset(dataset_path, image_indices=test_indices)"]},{"cell_type":"markdown","metadata":{"id":"_5on-Ys1bFi7"},"source":["# Set device to use GPU if available"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfQhwGp2a2r0"},"outputs":[],"source":["import torch\n","device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-5y0pJ4auqv"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"bOMsV6S3bQra"},"source":["# Load DINO Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BbMfU3hna-YH"},"outputs":[],"source":["# Load smallest dino model. ViT-S/8. Here ViT-S has ~22M parameters and\n","# works on 8x8 patches.\n","dino_model = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')\n","dino_model.eval().to(device)"]},{"cell_type":"markdown","metadata":{"id":"7TdAgxiNfRTi"},"source":["# Loop through DINO for all training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKBV8qQZbUqR"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from clip_dino import DINOSegmentation, compute_iou\n","from dino_model import DINOImageClassifier\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfIlrgoyoZ2_"},"outputs":[],"source":["from torchvision import transforms as T\n","\n","transform = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","\n","# transform = T.Compose([\n","#     # T.Resize((480, 480)),\n","#     T.ToTensor(),\n","#     T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","# ])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rriKMezNuImo"},"outputs":[],"source":["\n","# def get_dino_tokens(sample_image):\n","#     img_tensor = transform(sample_image)\n","#     # (3,480,480)\n","#     w, h = img_tensor.shape[1:]\n","#     img_tensor = img_tensor[None].to(device)\n","#     # (1,3,480,480)\n","\n","#     with torch.no_grad():\n","#         attn = dino_model.get_last_selfattention(img_tensor)[0, :, 0, 1:]\n","#         # (6,3600)\n","#         nh, tokens = attn.shape\n","#         w_feat, h_feat = w // 8, h // 8\n","#         attn = attn.reshape(nh, w_feat, h_feat)\n","#         attn = torch.nn.functional.interpolate(attn.unsqueeze(0), scale_factor=8, mode=\"nearest\")[0].cpu().numpy()\n","#         all_tokens = dino_model.get_intermediate_layers(img_tensor, n=1)[0]  # (1, 1+N, D)\n","#     # pprint(all_tokens)\n","#     return all_tokens.cpu()\n","\n","def get_dino_tokens_batch(X_batch):\n","\n","    # X_batch.shape = [batch_size,480,480,3]\n","\n","    X_batch = X_batch.float() / 255.0 # normalize\n","    # X_batch = X_batch.permute(0,3,1,2) # [N,H,W,C] -> [N,C,H,W]\n","    X_transform = torch.stack([\n","        transform(x) for x in X_batch\n","    ])\n","    X_transform = X_transform.to(device)\n","\n","    w, h = X_transform.shape[2:]\n","    # 480,480\n","\n","    with torch.no_grad():\n","        attn = dino_model.get_last_selfattention(X_transform)[:, :, 0, 1:]\n","        # (N,6,3600)\n","        # print(f\"attn.shape={attn.shape}\")\n","        nh, tokens = attn.shape[1:]\n","        w_feat, h_feat = w // 8, h // 8\n","        attn = attn.reshape(-1, nh, w_feat, h_feat)\n","        attn = torch.nn.functional.interpolate(attn, scale_factor=8, mode=\"nearest\").cpu().numpy()\n","        all_tokens = dino_model.get_intermediate_layers(X_transform, n=1)[0]  # (N, 1+pixels, D)\n","    return all_tokens.cpu()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgizHUoWBW-d"},"outputs":[],"source":["# How to convert from 480x480x1 to 1\n","def get_Y_labels(Y_batch):\n","    # input (N,480,480)\n","    # output (N)\n","\n","    # print(Y_batch.shape)\n","\n","    # use most common pixel classification for training\n","    Y_batch_modes = torch.stack([\n","        torch.bincount(Y_batch[i].flatten()).argmax()\n","        for i in range(Y_batch.shape[0])\n","    ])\n","\n","    return Y_batch_modes\n","\n","## test get_Y_labels\n","\n","# train_indices = [i for i in range(10)] # 10 images\n","# train_dataset = PetroSubImageDataset_v2(dataset_path, image_indices=train_indices)\n","\n","# train_dataloader = DataLoader(\n","#     train_dataset,\n","#     batch_size=7\n","# )\n","# batch = next(iter(train_dataloader))\n","# print(batch.shape)\n","# X_batch = batch[:,:-1]\n","# Y_batch = batch[:,-1]\n","# print(X_batch.shape,Y_batch.shape)\n","# print(get_Y_labels(Y_batch))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mcxmAIQg1nh"},"outputs":[],"source":["num_classes = 18\n","batch_size = 32\n","num_iters = 50\n","hidden_dim = 768\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=batch_size\n",")\n","\n","model = DINOImageClassifier().to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","train_ious = []\n","val_ious = []\n","\n","for iter in range(num_iters):\n","\n","    ### TRAINING ###\n","\n","    model.train()\n","    total_train_loss = 0.0\n","    correct_train = 0.0\n","    total_train = 0.0\n","    total_train_iou = 0.0\n","    pbar = tqdm(train_dataloader, desc=f\"Train Epoch {iter+1}/{num_iters}\")\n","    for batch in pbar:\n","\n","        X_batch = batch[:,:-1]\n","        Y_batch = batch[:,-1]\n","\n","        Y_labels = get_Y_labels(Y_batch).to(device)\n","        # print(f\"X: {X_batch.shape}, Y: {Y_labels.shape}\")\n","\n","        with torch.no_grad():\n","            X_tokens = get_dino_tokens_batch(X_batch) # [N, 3601, D=384]\n","            X_tokens = X_tokens[:, 1:, :]  # (N, 3600, D) # uncomment to drop CLS token\n","\n","        # print(f\"X: {X_batch.shape}->{X_tokens.shape}, Y: {Y_batch.shape}->{Y_labels.shape}\")\n","\n","        X = X_tokens.to(device)\n","        Y = Y_labels.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(X)\n","        loss = loss_fn(logits, Y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        batch_loss = loss.item()\n","        total_train_loss += batch_loss\n","\n","        preds = torch.argmax(logits, dim=1)\n","        correct_train += (preds == Y).sum().item()\n","        total_train += Y.size(0)\n","\n","        batch_iou = compute_iou(preds.cpu().numpy(), Y.cpu().numpy(), num_classes)\n","        total_train_iou += batch_iou\n","\n","        pbar.set_postfix(loss=f\"{batch_loss:.2f}\", iou=f\"{batch_iou:.2f}\")\n","\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","    avg_train_accuracy = correct_train / total_train\n","    avg_train_iou = total_train_iou / len(train_dataloader)\n","\n","    train_losses.append(avg_train_loss)\n","    train_accuracies.append(avg_train_accuracy)\n","    train_ious.append(avg_train_iou)\n","\n","    # print(f\"Epoch {iter+1}: Train Loss={avg_train_loss:.2f}, Accuracy={avg_train_accuracy:.2f}, IoU={avg_train_iou:.2f}\")\n","\n","    ### VALIDATION ###\n","\n","    model.eval()\n","    total_val_loss = 0.0\n","    correct_val = 0.0\n","    total_val = 0.0\n","    total_val_iou = 0.0\n","    with torch.no_grad():\n","\n","        pbar = tqdm(test_dataloader, desc=f\"Val Epoch {iter+1}/{num_iters}\")\n","\n","        for batch in pbar:\n","            X_batch = batch[:, :-1]\n","            Y_batch = batch[:, -1]\n","            Y_labels = get_Y_labels(Y_batch).to(device)\n","\n","            X_tokens = get_dino_tokens_batch(X_batch) # [N, 3601, D=384]\n","            X_tokens = X_tokens[:, 1:, :]  # (N, 3600, D) # uncomment to drop CLS token\n","            X = X_tokens.to(device)\n","            Y = Y_labels.to(device)\n","\n","            logits = model(X)\n","            loss = loss_fn(logits, Y)\n","            val_loss = loss.item()\n","            total_val_loss += val_loss\n","\n","            preds = torch.argmax(logits, dim=1)\n","            correct_val += (preds == Y).sum().item()\n","            total_val += Y.size(0)\n","\n","            batch_iou = compute_iou(preds.cpu().numpy(), Y.cpu().numpy(), num_classes)\n","            total_val_iou += batch_iou\n","\n","            pbar.set_postfix(loss=f\"{val_loss:.2f}\", iou=f\"{batch_iou:.2f}\")\n","\n","\n","    avg_val_loss = total_val_loss / len(test_dataloader)\n","    avg_val_accuracy = correct_val / total_val\n","    avg_val_iou = total_val_iou / len(test_dataloader)\n","\n","    val_losses.append(avg_val_loss)\n","    val_accuracies.append(avg_val_accuracy)\n","    val_ious.append(avg_val_iou)\n","    # print(f\"Epoch {iter+1}: Val Loss={avg_val_loss:.2f}, Accuracy={avg_val_accuracy:.2f}, IoU={avg_val_iou:.2f}\")\n","\n","    print()\n","\n","    print(f\"Epoch {iter+1}: Train Loss={avg_train_loss:.2f}, Accuracy={avg_train_accuracy:.2f} // Val Loss={avg_val_loss:.2f}, Accuracy={avg_val_accuracy:.2f}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_qdKlKxh-ms"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","epochs = range(1, num_iters + 1)\n","\n","plt.figure(figsize=(18, 5))\n","\n","plt.subplot(1, 3, 1)\n","plt.plot(epochs, train_losses, label=\"Train Loss\")\n","plt.plot(epochs, val_losses, label=\"Val Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss Curve\")\n","plt.legend()\n","\n","plt.subplot(1, 3, 2)\n","plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n","plt.plot(epochs, val_accuracies, label=\"Val Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Accuracy Curve\")\n","plt.legend()\n","\n","\n","plt.subplot(1, 3, 3)\n","plt.plot(epochs, train_ious, label=\"Train IoU\")\n","plt.plot(epochs, val_ious, label=\"Val IoU\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"IoU\")\n","plt.title(\"IoU Curve\")\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","source":["# Train for BIOTIC OR NOT"],"metadata":{"id":"jB3jhijbShFw"}},{"cell_type":"code","source":["# Lets use Biotic/Abiotic only\n","\n","# Abiotic\n","# -  1 - Micrite\n","# -  2 - Cement\n","# - 11 - Ooids\n","# - 17 - Other Abiotic\n","\n","# Biotic\n","# -  3 - Peloid/Pellet\n","# -  4 - Protist\n","# -  5 - Microbial\n","# -  6 - Mollusk\n","# -  7 - Echinoderm\n","# -  9 - Coral\n","# - 10 - Aglae\n","# - 12 - Biotic\n","# - 13 - Ostracod\n","# - 14 - Trilobite\n","# - 15 - Bryozoan\n","# - 16 - Sponge\n","\n","\n","# Scale Bar\n","# - 8 - Scale Bar\n","\n","\n","def get_Y_biotic_ornot(Y_batch):\n","\n","    biotic_ornot_mask = np.full_like(Y_batch, fill_value=3, dtype=np.uint8)\n","\n","    abiotic = {1, 2, 11, 17}\n","    biotic = {3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16}\n","    scale_bar = {8}\n","\n","    biotic_ornot_mask[np.isin(Y_batch, list(abiotic))] = 0\n","    biotic_ornot_mask[np.isin(Y_batch, list(biotic))] = 1\n","    biotic_ornot_mask[np.isin(Y_batch, list(scale_bar))] = 2\n","\n","    any_biotic = (biotic_ornot_mask == 1).any(axis=(1, 2)).astype(np.uint8)\n","\n","\n","    return torch.tensor(any_biotic)\n","\n","## test get_Y_labels\n","\n","# train_indices = [i for i in range(20)] # N=20 images\n","# train_dataset = PetroSubImageDataset(dataset_path, image_indices=train_indices)\n","\n","# train_dataloader = DataLoader(\n","#     train_dataset,\n","#     batch_size=20\n","# )\n","# for batch in train_dataloader:\n","#     print(batch.shape)\n","#     X_batch = batch[:,:-1]\n","#     Y_batch = batch[:,-1]\n","#     print(X_batch.shape,Y_batch.shape)\n","#     biotic_ornot = get_Y_biotic_ornot(Y_batch)\n","#     print(biotic_ornot.shape)\n","#     print(biotic_ornot)"],"metadata":{"id":"UmpmyRAPDNw_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = 2 # 18\n","batch_size = 32\n","num_iters = 50\n","hidden_dim = 768\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=batch_size\n",")\n","\n","model = DINOImageClassifier().to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","train_ious = []\n","val_ious = []\n","\n","for iter in range(num_iters):\n","\n","    ### TRAINING ###\n","\n","    model.train()\n","    total_train_loss = 0.0\n","    correct_train = 0.0\n","    total_train = 0.0\n","    total_train_iou = 0.0\n","    pbar = tqdm(train_dataloader, desc=f\"Train Epoch {iter+1}/{num_iters}\")\n","    for batch in pbar:\n","\n","        X_batch = batch[:,:-1]\n","        Y_batch = batch[:,-1]\n","\n","        Y_labels = get_Y_biotic_ornot(Y_batch).to(device)\n","        # print(f\"X: {X_batch.shape}, Y: {Y_labels.shape}\")\n","\n","        with torch.no_grad():\n","            X_tokens = get_dino_tokens_batch(X_batch) # [N, 3601, D=384]\n","            X_tokens = X_tokens[:, 1:, :]  # (N, 3600, D) # uncomment to drop CLS token\n","\n","        # print(f\"X: {X_batch.shape}->{X_tokens.shape}, Y: {Y_batch.shape}->{Y_labels.shape}\")\n","\n","        X = X_tokens.to(device)\n","        Y = Y_labels.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(X)\n","        loss = loss_fn(logits, Y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        batch_loss = loss.item()\n","        total_train_loss += batch_loss\n","\n","        preds = torch.argmax(logits, dim=1)\n","        correct_train += (preds == Y).sum().item()\n","        total_train += Y.size(0)\n","\n","        batch_iou = compute_iou(preds.cpu().numpy(), Y.cpu().numpy(), num_classes)\n","        total_train_iou += batch_iou\n","\n","        pbar.set_postfix(loss=f\"{batch_loss:.2f}\", iou=f\"{batch_iou:.2f}\")\n","\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","    avg_train_accuracy = correct_train / total_train\n","    avg_train_iou = total_train_iou / len(train_dataloader)\n","\n","    train_losses.append(avg_train_loss)\n","    train_accuracies.append(avg_train_accuracy)\n","    train_ious.append(avg_train_iou)\n","\n","    # print(f\"Epoch {iter+1}: Train Loss={avg_train_loss:.2f}, Accuracy={avg_train_accuracy:.2f}, IoU={avg_train_iou:.2f}\")\n","\n","    ### VALIDATION ###\n","\n","    model.eval()\n","    total_val_loss = 0.0\n","    correct_val = 0.0\n","    total_val = 0.0\n","    total_val_iou = 0.0\n","    with torch.no_grad():\n","\n","        pbar = tqdm(test_dataloader, desc=f\"Val Epoch {iter+1}/{num_iters}\")\n","\n","        for batch in pbar:\n","            X_batch = batch[:, :-1]\n","            Y_batch = batch[:, -1]\n","            Y_labels = get_Y_biotic_ornot(Y_batch).to(device)\n","\n","            X_tokens = get_dino_tokens_batch(X_batch) # [N, 3601, D=384]\n","            X_tokens = X_tokens[:, 1:, :]  # (N, 3600, D) # uncomment to drop CLS token\n","            X = X_tokens.to(device)\n","            Y = Y_labels.to(device)\n","\n","            logits = model(X)\n","            loss = loss_fn(logits, Y)\n","            val_loss = loss.item()\n","            total_val_loss += val_loss\n","\n","            preds = torch.argmax(logits, dim=1)\n","            correct_val += (preds == Y).sum().item()\n","            total_val += Y.size(0)\n","\n","            batch_iou = compute_iou(preds.cpu().numpy(), Y.cpu().numpy(), num_classes)\n","            total_val_iou += batch_iou\n","\n","            pbar.set_postfix(loss=f\"{val_loss:.2f}\", iou=f\"{batch_iou:.2f}\")\n","\n","\n","    avg_val_loss = total_val_loss / len(test_dataloader)\n","    avg_val_accuracy = correct_val / total_val\n","    avg_val_iou = total_val_iou / len(test_dataloader)\n","\n","    val_losses.append(avg_val_loss)\n","    val_accuracies.append(avg_val_accuracy)\n","    val_ious.append(avg_val_iou)\n","    # print(f\"Epoch {iter+1}: Val Loss={avg_val_loss:.2f}, Accuracy={avg_val_accuracy:.2f}, IoU={avg_val_iou:.2f}\")\n","\n","    print()\n","\n","    print(f\"Epoch {iter+1}: Train Loss={avg_train_loss:.2f}, Accuracy={avg_train_accuracy:.2f} // Val Loss={avg_val_loss:.2f}, Accuracy={avg_val_accuracy:.2f}\")\n","\n"],"metadata":{"id":"eP3s-u2kC_qX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","epochs = range(1, num_iters + 1)\n","\n","plt.figure(figsize=(18, 5))\n","\n","plt.subplot(1, 3, 1)\n","plt.plot(epochs, train_losses, label=\"Train Loss\")\n","plt.plot(epochs, val_losses, label=\"Val Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss Curve\")\n","plt.legend()\n","\n","plt.subplot(1, 3, 2)\n","plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n","plt.plot(epochs, val_accuracies, label=\"Val Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Accuracy Curve\")\n","plt.legend()\n","\n","\n","plt.subplot(1, 3, 3)\n","plt.plot(epochs, train_ious, label=\"Train IoU\")\n","plt.plot(epochs, val_ious, label=\"Val IoU\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"IoU\")\n","plt.title(\"IoU Curve\")\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"GT0uQL9zTSyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GnRLjvrrd0t_"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}